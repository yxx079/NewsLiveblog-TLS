# NewsLiveblogâ€‘TLSÂ ğŸ—ï¸â±ï¸

*A Timeâ€‘Sensitive Benchmark for Extractive TimelineÂ Summarization*

---

## 1Â Â Project at a Glance

| Item                | Value                                                                     |
| ------------------- | ------------------------------------------------------------------------- |
| **Language**        | Â English                                                                  |
| **Domain**          | Â Breaking / World News (*TheÂ Guardian* liveblogs)                         |
| **Primary Task**    | Â ExtractiveÂ TimelineÂ Summarization (TLS)                                  |
| **Secondary Tasks** | Â Keyâ€‘event detection Â· Temporal sentence classification                   |

---

## 2Â Â Data Format & Folder Layout

```
NewsLiveblog-TLS/
â”œâ”€â”€ data/
â”‚Â Â  â”œâ”€â”€ world_news.json     # 1â€¯473 liveblogs (â‰ˆâ€¯124â€¯k sentences)
â”‚Â Â  â”œâ”€â”€ data_collection.ipynb  # HTML scraping via TheGuardian API
â”‚Â Â  â””â”€â”€ preprocess.py          # sentenceâ€‘level oracle labelling (TAEGS)
â””â”€â”€ README.md
```

Each line in `*.jsonl` is a **single liveblog** (UTFâ€‘8, noÂ BOM):

```json
{
  "document":   ["sentence_1", â€¦, "sentence_N"],
  "timeline":   ["YYYYâ€‘MMâ€‘DDThh:mm:ssZ", â€¦] ,   # length = N
  "summary":    ["editor bulletâ€‘1", â€¦],          # keyâ€‘point list
  "key_timeline": ["ISOâ€‘8601â€‘1", â€¦]              # oracle dates
}
```

---

## 3Â Â Construction Pipeline

| Stage                    | Tool / Script            | Key Decisions                                                                                                                                      |         |   |         |               |
| ------------------------ | ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | - | ------- | ------------- |
| **1Â Â Retrieval**         | Â `data_collection.ipynb` | Queried *TheÂ Guardian* ContentÂ API (worldÂ news, typeÂ =Â "liveblog"), fetched latestÂ 2â€¯000 posts (2012â€¯â†’â€¯2024).                                      |         |   |         |               |
| **2Â Â HTMLÂ Parse**        | Â BeautifulSoup           | Kept `<div class="block-elements">`; removed nav, ads, footers.                                                                                    |         |   |         |               |
| **3Â Â NoiseÂ Reduction**   | regex rules              | Dropped multiâ€‘topic â€œlive updatesâ€, sports blogs, inâ€‘progress threads.                                                                             |         |   |         |               |
| **4Â Â ContentÂ Trim**      | heuristics               | Removed titles, image captions, Twitter embeds, â€œEmailÂ linkâ€ strings; kept finished blogs only.                                                    |         |   |         |               |
| **5Â Â AdvancedÂ Cleaning** | wordâ€‘ratio filter        | Deleted rows where **compression ratio** (                                                                                                         | summary | / | article | words)â€¯>â€¯0.4. |
| **6Â Â OracleÂ Labelling**  | Â `preprocess.py` (TAEGS) | spaCy sentence splitâ€¯+â€¯NER (`DATE`) â†’ greedy maximise **ROUGEâ€‘L** under dateâ€‘overlap constraint; outputs `extract_label` indices + `key_timeline`. |         |   |         |               |

> **TAEGS** (Temporalâ€‘Alignment Enhanced GreedyÂ Selection) is fully described in report Â§4.3; pseudocode in *preprocess.py* matches AlgorithmÂ 1. The final corpus contains **1â€¯473** cleaned liveblogs.

---

## 4Â Â Corpus Statistics

| Metric                         | Value                               |
| ------------------------------ | ----------------------------------- |
| Documents                      | **1â€¯473**                           |
| **Avgâ€¯tokens / doc**           | Â 2â€¯281.21                           |
| **Avgâ€¯sentences / doc**        | Â 89.50                              |
| **Avgâ€¯dates / doc**            | Â 13.27                              |
| **Avgâ€¯sentences / date**       | Â 6.74                               |
| **Avgâ€¯summary tokens**         | Â 151.40                             |
| **Avgâ€¯summary sentences**      | Â 15.04                              |
| **Sentenceâ€‘level compression** | Â Ã—â€¯9.16                             |
| **Dateâ€‘level compression**     | Â Ã—â€¯4.08                             |
| VocabularyÂ (doc / summary)     | Â 79â€¯866Â /Â 16â€¯140 unique tokens      |
| Time span per liveblog         | Â meanÂ â‰ˆâ€¯4â€¯hÂ 41â€¯m (maxÂ <â€¯12â€¯h)       |
| Coverage years                 | Â 2012â€¯â€“â€¯2024 (peaksÂ inÂ 2014â€¯&â€¯2022) |

---

## 5Â Â Suggested Tasks & Metrics

| Task                       | Input                   | Output                       | Recommended Metrics               |
| -------------------------- | ----------------------- | ---------------------------- | --------------------------------- |
| **TimelineÂ Summarization** | `document` + `timeline` | pick *k* sentences per date  | ROUGEâ€‘1/2/L Â· **Dateâ€‘F1** Â· AR1â€‘F |
| **Keyâ€‘Event Detection**    | same                    | predict `key_timeline` dates | Dateâ€‘F1,Â MeanÂ Delay               |
| **Temporal SentenceÂ Cls.** | single sentence         | binary label                 | F1â€‘macro                          |

---

## 6Â Â QuickÂ Start (PyTorch)

```python
import json, pathlib
from transformers import AutoTokenizer, AutoModel

tok = AutoTokenizer.from_pretrained("bert-base-uncased")
bert = AutoModel.from_pretrained("bert-base-uncased")

with pathlib.Path("world_news.json").open() as f:
    sample = json.loads(f.readline())

print("Sentences:", len(sample["document"]))
print("First sentence:", sample["document"][0])
print("First timestamp:", sample["timeline"][0])

enc = tok(sample["document"][:16],
          padding=True, truncation=True,
          max_length=32, return_tensors="pt")
out = bert(**enc)
print(out.last_hidden_state.shape)      # (batch, seq, 768)
```

---

## 7Â Â License

This dataset redistributes text snippets from **TheÂ Guardian** under UK *fair dealing* / US *fair use* for **nonâ€‘commercial research** only.
For commercial usage please obtain permission from the original publisher.



### Acknowledgements

Data collection scripts build upon *TheÂ Guardian* OpenÂ API. Sentence splitting &Â NER use **spaCyÂ 3.7** (`en_core_web_sm`). Temporal alignment oracle is implemented in *preprocess.py* (see Â§4.3 of the accompanying report).
